{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4631e8c0-6dd8-4e7c-9590-f2f7f013e2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------+-------+--------------------+-----+-----------+------+------+-----+--------+----------------+-----+-----------+----------+---------------+\n",
      "| Age|Cabin|Embarked|   Fare|                Name|Parch|PassengerId|Pclass|   Sex|SibSp|Survived|          Ticket|Title|Family_Size|new_column|choose_a_column|\n",
      "+----+-----+--------+-------+--------------------+-----+-----------+------+------+-----+--------+----------------+-----+-----------+----------+---------------+\n",
      "|22.0| NULL|       S|   7.25|Braund, Mr. Owen ...|    0|          1|     3|  male|    1|     0.0|       A/5 21171|   Mr|          1|      NULL|           NULL|\n",
      "|38.0|  C85|       C|71.2833|Cumings, Mrs. Joh...|    0|          2|     1|female|    1|     1.0|        PC 17599|  Mrs|          1|      NULL|           NULL|\n",
      "|26.0| NULL|       S|  7.925|Heikkinen, Miss. ...|    0|          3|     3|female|    0|     1.0|STON/O2. 3101282| Miss|          0|      NULL|           NULL|\n",
      "|35.0| C123|       S|   53.1|Futrelle, Mrs. Ja...|    0|          4|     1|female|    1|     1.0|          113803|  Mrs|          1|      NULL|           NULL|\n",
      "+----+-----+--------+-------+--------------------+-----+-----------+------+------+-----+--------+----------------+-----+-----------+----------+---------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-04-27 15:29:...|6711211621056771738|2707077687595228535|               true|\n",
      "|2024-04-27 15:27:...|2707077687595228535|8059457254979550641|               true|\n",
      "|2024-04-27 15:26:...|8059457254979550641|4316376306380043017|               true|\n",
      "|2024-04-27 15:25:...|4316376306380043017|6945889682975321469|               true|\n",
      "|2024-04-27 15:25:...|6945889682975321469|2500024997980005591|               true|\n",
      "|2024-04-27 15:23:...|2500024997980005591|9015970749471366070|               true|\n",
      "|2024-04-27 15:13:...|9015970749471366070|1036892303015044680|               true|\n",
      "|2024-04-27 15:11:...|1036892303015044680|3663590870506627609|               true|\n",
      "|2024-04-27 13:29:...|3663590870506627609| 922663229200831164|               true|\n",
      "|2024-04-27 13:28:...| 922663229200831164|9220030009765536769|               true|\n",
      "|2024-04-27 10:37:...|9220030009765536769|2195253128215160933|               true|\n",
      "|2024-04-27 10:36:...|2195253128215160933|               NULL|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'FIELD': missing '('.(line 1, pos 42)\n\n== SQL ==\nALTER TABLE default.titanic ADD PARTITION FIELD bucket(Age);\n------------------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# show snapshots details\u001b[39;00m\n\u001b[1;32m     20\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM default.titanic.history ORDER BY made_current_at DESC;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 22\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALTER TABLE default.titanic ADD PARTITION FIELD bucket(Age);\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# time travel to timestamp\u001b[39;00m\n\u001b[1;32m     25\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM default.titanic TIMESTAMP AS OF \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-04-27T15:13:00.289+00:00\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/iceberg-practice/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/code/iceberg-practice/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/code/iceberg-practice/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'FIELD': missing '('.(line 1, pos 42)\n\n== SQL ==\nALTER TABLE default.titanic ADD PARTITION FIELD bucket(Age);\n------------------------------------------^^^\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set the absolute paths to the Iceberg tables and JAR files\n",
    "iceberg_tables_path = \"/Users/france.cama/code/iceberg-practice/iceberg_tables\"\n",
    "iceberg_jars_path = \"/Users/france.cama/code/iceberg-practice/jars/iceberg-spark-runtime-3.5_2.13-1.5.0.jar\"\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg time travel feature\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dderby.system.home=\" + iceberg_tables_path) \\\n",
    "    .config(\"spark.jars\", iceberg_jars_path) \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_tables_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM default.titanic;\").show(4)\n",
    "\n",
    "# show snapshots details\n",
    "spark.sql(\"SELECT * FROM default.titanic.history ORDER BY made_current_at DESC;\").show()\n",
    "\n",
    "# modify partitioning schema\n",
    "df.updateSpec()\n",
    "    .addField(bucket(\"Age\", 5))\n",
    "    .commit();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d8dd5d-1a24-4279-b074-385f20981a49",
   "metadata": {},
   "source": [
    "Iceberg table partitioning logic can be updated in an existing table because queries do not reference partition values directly.\n",
    "When you evolve a partition spec, the old data written with an earlier spec remains unchanged (are not physically rewritten on the disk). New data is written using the new spec in a new layout. Metadata for each of the partition versions is kept separately. Because of this, when you start writing queries, you get split planning: \n",
    "\n",
    "<img src=\"./images/partition-spec-evolution.png\" style=\"max-width: 50%;\"></img>\n",
    "The data for 2008 is partitioned by month. Starting from 2009 the table is updated so that the data is instead partitioned by day. Both partitioning layouts are able to coexist in the same table.\n",
    "\n",
    "#### Iceberg vs Hive table format\n",
    "- When making a query using iceberg you don't have to know which is the partitioning layout of a table, thanks to the hidden partitioning feature.\n",
    "- iceberg partition layouts can evolve as needed.\n",
    "- iceberg handles partition logic values on its own, in Hive you have to write the query correctly and working queries are tied to the table's partitioning scheme, so partitioning configuration cannot be changed without breaking queries.\n",
    "\n",
    "Most importantly, queries no longer depend on a table's physical layout. With a separation between physical and logical, Iceberg tables can evolve partition schemes over time as data volume changes. Iceberg does not require costly distractions, like rewriting table data or migrating to a new table.\n",
    "Partitioning schema is contained in the metadata file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabd9e6-9e11-481c-8999-a686a142b144",
   "metadata": {},
   "source": [
    "#### Sort order evolution\n",
    "Similar to partition spec, Iceberg sort order can also be updated in an existing table. When you evolve a sort order, the old data written with an earlier order remains unchanged. Engines can always choose to write data in the latest sort order or unsorted when sorting is prohibitively expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293c3cd-0152-4257-b713-747e19b11d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
