{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af023974-7b25-49c6-91a9-62c6ddb509e8",
   "metadata": {},
   "source": [
    "Apache Iceberg provides versioning and time travel capabilities, allowing users to query data as it existed at a specific point in time. This feature can be extremely useful for debugging, auditing, and historical analysis.\n",
    "\n",
    "Time travel in Iceberg allows users to access historical snapshots of their table. Snapshots are created whenever a table is modified, such as adding or deleting data, and are assigned unique identifiers. Each snapshot is a consistent and complete view of the table at a given point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08ff5cdc-baa2-4bb6-a123-d140317e3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/22 11:49:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Catalog: spark_catalog\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-07-22 11:47:...|6609739537041086161|1184812391625634863|               true|\n",
      "|2024-07-22 11:11:...|1759033321787871743| 813399347516847428|              false|\n",
      "|2024-07-22 11:11:...| 813399347516847428|6609739537041086161|              false|\n",
      "|2024-07-22 11:11:...|6609739537041086161|1184812391625634863|               true|\n",
      "|2024-07-22 11:10:...|1184812391625634863|4619981039604578990|               true|\n",
      "|2024-07-22 11:09:...|4619981039604578990|               NULL|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "+----+-----+--------+-------+--------------------+-----+-----------+------+------+-----+--------+----------------+-----+-----------+---------------+-------+\n",
      "| Age|Cabin|Embarked|   Fare|                Name|Parch|PassengerId|Pclass|   Sex|SibSp|Survived|          Ticket|Title|Family_Size|choose_a_column|new_col|\n",
      "+----+-----+--------+-------+--------------------+-----+-----------+------+------+-----+--------+----------------+-----+-----------+---------------+-------+\n",
      "|77.0|    a|       b|   77.0|                 ccc|    1|          2|     3|     M|    4|    77.0|               a|    b|          4|          lorem|  lorem|\n",
      "|77.0|    a|       b|   77.0|                 ccc|    1|          2|     3|     M|    4|    77.0|               a|    b|          4|          lorem|   NULL|\n",
      "|22.0| NULL|       S|   7.25|Braund, Mr. Owen ...|    0|          1|     3|  male|    1|     0.0|       A/5 21171|   Mr|          1|           NULL|   NULL|\n",
      "|38.0|  C85|       C|71.2833|Cumings, Mrs. Joh...|    0|          2|     1|female|    1|     1.0|        PC 17599|  Mrs|          1|           NULL|   NULL|\n",
      "|26.0| NULL|       S|  7.925|Heikkinen, Miss. ...|    0|          3|     3|female|    0|     1.0|STON/O2. 3101282| Miss|          0|           NULL|   NULL|\n",
      "+----+-----+--------+-------+--------------------+-----+-----------+------+------+-----+--------+----------------+-----+-----------+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-----+--------+-------+--------------------+-----+-----------+------+------+-----+--------+----------------+-----+-----------+\n",
      "| Age|Cabin|Embarked|   Fare|                Name|Parch|PassengerId|Pclass|   Sex|SibSp|Survived|          Ticket|Title|Family_Size|\n",
      "+----+-----+--------+-------+--------------------+-----+-----------+------+------+-----+--------+----------------+-----+-----------+\n",
      "|22.0| NULL|       S|   7.25|Braund, Mr. Owen ...|    0|          1|     3|  male|    1|     0.0|       A/5 21171|   Mr|          1|\n",
      "|38.0|  C85|       C|71.2833|Cumings, Mrs. Joh...|    0|          2|     1|female|    1|     1.0|        PC 17599|  Mrs|          1|\n",
      "|26.0| NULL|       S|  7.925|Heikkinen, Miss. ...|    0|          3|     3|female|    0|     1.0|STON/O2. 3101282| Miss|          0|\n",
      "|35.0| C123|       S|   53.1|Futrelle, Mrs. Ja...|    0|          4|     1|female|    1|     1.0|          113803|  Mrs|          1|\n",
      "|35.0| NULL|       S|   8.05|Allen, Mr. Willia...|    0|          5|     3|  male|    0|     0.0|          373450|   Mr|          0|\n",
      "+----+-----+--------+-------+--------------------+-----+-----------+------+------+-----+--------+----------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'CALL'.(line 1, pos 0)\n\n== SQL ==\nCALL spark_catalog.system.rollback_to_snapshot('default.titanic', 6609739537041086161)\n^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m### ROLL BACK ###\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Roll back an Iceberg table to a previous snapshot using either a timestamp or a snapshot ID.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Users who have been assigned the ADMIN role, the table's owner, and users with INSERT, UPDATE, or DELETE privileges on the table can use the ROLLBACK command.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# ROLLBACK TABLE default.titanic TO { SNAPSHOT '6609739537041086161'};\u001b[39;00m\n\u001b[1;32m     37\u001b[0m SNAPSHOT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6609739537041086161\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCALL spark_catalog.system.rollback_to_snapshot(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdefault.titanic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSNAPSHOT\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTITANIC ROLLED BACK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM titanic;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/v1/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/v1/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/v1/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'CALL'.(line 1, pos 0)\n\n== SQL ==\nCALL spark_catalog.system.rollback_to_snapshot('default.titanic', 6609739537041086161)\n^^^\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set the absolute paths to the Iceberg tables and JAR files\n",
    "iceberg_tables_path = \"/Users/france.cama/code/iceberg-practice/iceberg_tables\"\n",
    "iceberg_jars_path = \"/Users/france.cama/code/iceberg-practice/jars/iceberg-spark-runtime-3.5_2.12-1.5.1.jar\"\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg time travel feature\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dderby.system.home=\" + iceberg_tables_path) \\\n",
    "    .config(\"spark.jars\", iceberg_jars_path) \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_tables_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "current_catalog = spark.catalog.currentCatalog()\n",
    "print(f\"Current Catalog: {current_catalog}\")\n",
    "# show snapshots details\n",
    "spark.sql(\"SELECT * FROM default.titanic.history ORDER BY made_current_at DESC;\").show()\n",
    "\n",
    "# time travel to timestamp\n",
    "df = spark.sql(f\"SELECT * FROM default.titanic TIMESTAMP AS OF '2024-07-22T11:10:00.289+00:00';\")\n",
    "df.show(5)\n",
    "\n",
    "# time travel using snapshot_id\n",
    "df_id = spark.sql(f\"SELECT * FROM default.titanic VERSION AS OF 4619981039604578990;\")\n",
    "df_id.show(5)\n",
    "\n",
    "\n",
    "### ROLL BACK ###\n",
    "# Roll back an Iceberg table to a previous snapshot using either a timestamp or a snapshot ID.\n",
    "# Users who have been assigned the ADMIN role, the table's owner, and users with INSERT, UPDATE, or DELETE privileges on the table can use the ROLLBACK command.\n",
    "# ROLLBACK TABLE default.titanic TO { SNAPSHOT '6609739537041086161'};\n",
    "SNAPSHOT = 6609739537041086161\n",
    "spark.sql(f\"CALL spark_catalog.system.rollback_to_snapshot('default.titanic', {SNAPSHOT})\")\n",
    "\n",
    "print(\"TITANIC ROLLED BACK\")\n",
    "spark.sql(\"SELECT * FROM titanic;\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a736d0-b2fe-46c4-b07c-aed61c829912",
   "metadata": {},
   "source": [
    "## Time travel query lifecycle regarding metadata.\n",
    "\n",
    "Iceberg first looks for the snapshot_id selected in the snapshot JSON object in the metadata file. In this object several properties are contained like: parent-snapshot-id, timestamp-ms, manifest-list, id ecc. Using this metadata than the manifest-list file is retrieved. In this file several informations are included and, in particular, the manifest file(s) (in the manifest_path). By reading this manifest files can use several statistics on the data columns but, more important, the data files path to select on which the data are contained and the query is interested in. \n",
    "In conclusion, by leveraging table metadata and 'going straight to the point', iceberg retrieve old data while offering good performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
